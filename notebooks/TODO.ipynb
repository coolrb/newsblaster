{
 "metadata": {
  "name": "TODO"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Newsblaster Components\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Crawling (RSS, API)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Target sites: http://www.4imn.com/top200/"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "TODO:\n",
      "[ ] Find english versions of these\n",
      "[ ] Determine flexibility of API/RSS feed, NOTE BENE: DO WE NEED TO SCRAPE PAGE DIRECTLY?\n",
      " \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Article Extraction (ML or rulebased)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "TODO:\n",
      "[ ] port article extractor java 2 python\n",
      "[ ] annotate data\n",
      "[ ] meta-data extraction\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Article Storage and IR"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "TODO:\n",
      "[ ] Set up ingestion engine that works on a daily/hourly basis with rss/api feeds\n",
      "[ ] Find location to store this data (possibly one of the coral's or new machines)\n",
      "[ ] Separate out content stores from annotation stores\n",
      "[ ] IR indexing of content (ElasticSearch or other),\n",
      "  \\--[ ] Can we have iterative tf/idf on demand?\n",
      "   --[ ] language modeling on demand?\n",
      "   --[ ] web interface to observe how much data we are crawling per day\n",
      "   --[ ] unified-meta data \n",
      "\n",
      "[ ] ingest old archive storage\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}